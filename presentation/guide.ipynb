{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <center>In The Name of God</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<div style=\"text-align: justify\">In this tutorial we are going to cover some important rules in programing with **tensorflow** in python. this tutorial includes basic parts for understanding the logic behind the rules.</div>\n",
    "\n",
    "### Statical Graph **V.S.** Eager Execution\n",
    "<div style=\"text-align: justify\">Tensorflow used to work with Statical Graph. which means tensorFlow uses a dataflow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you first define the dataflow graph, then create a tensorFlow session to run parts of the graph across a set of local and remote devices.</div> \n",
    "\n",
    "<img src=files/tensors_flowing.gif>\n",
    "\n",
    "<div style=\"text-align: justify\">On the other hand tensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with tensorflow and debug models, and also build complex models.</div> \n",
    "\n",
    "<div style=\"text-align: justify\">The eager execution mode was introduced after tensorflow 1 and it seems that it will be tensorflow's default mode after tensorflow 2.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# session mode\n",
    "print(tf.executing_eagerly())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">**Now restart the kernel**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# eager mode\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can still use graph and sessions in eager_execution mode. the only difference is that we do not have default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.executing_eagerly())\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many differents between session and eager mode in tensorflow. some of them are as fallow:\n",
    "* In session mode most of python methods execute once or twice or so on, due to they are forming the graph. On the contrary, in eager mode methods execute many times (for each batch).\n",
    "* In eager mode, variables do not have special meaning and are not tracked. So you can update every tensor for optimizing a loss using [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(5)\n",
    "print(tf.global_variables())\n",
    "\n",
    "with g.as_default():\n",
    "    v = tf.Variable(5)\n",
    "    print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensors are python objects in eager execution so they can be easily change to other formats, be deleted and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "x = x + 1  # previous x was deleted\n",
    "\n",
    "with g.as_default():\n",
    "    x = tf.constant(1)  # x points to first Tensor\n",
    "    x = x + 1  # now x points to second Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ...\n",
    "\n",
    "\n",
    "Read more about [graphs and sessions](https://www.tensorflow.org/guide/graphs) and [eager execution](https://www.tensorflow.org/guide/eager)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "# Methods Standards\n",
    "\n",
    "In writing a method there are three standards that should be put under considerations\n",
    "* method information (including description, args, returns, raises)\n",
    "* name scope\n",
    "\n",
    "### Method Information\n",
    "Method informaiton should be complete and reproducible. It must include:\n",
    "* Method description\n",
    "* Input arguments, including tensors shape and types if they are important \n",
    "* Returns, including shape, type, ...\n",
    "* Raises: if method checks some exception(s) explicitly\n",
    "\n",
    "### name scope\n",
    "<div style=\"text-align: justify\">since in the statical graph programing names play an important role in low levl API's (variables and operations can be called by their names), working correctly with names can help us to have a better code in low level API, and name scopes can make our model modular since we can call all nodes with specific name scope or recall, save or update variables with same scopes.</div>\n",
    "\n",
    "In defining methods we should adhere to three rules:\n",
    "* Choose a name for the method.\n",
    "* Put all operations under that name scope.\n",
    "* Name the last tensor (which is going to be retured) that specific name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensordot(A, B, name=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns the sum of element wise multiplication of A and B tensors\n",
    "\n",
    "    Args:\n",
    "        A: an arbitrary Tensor\n",
    "        B: a Tensor with same shape as A\n",
    "        name: (Optional)\n",
    "        \n",
    "    Returns:\n",
    "        a scalar which is sum of element wise multiplication of A and B tensors\n",
    "        \n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"tensordot\"\n",
    "    with tf.name_scope(name):\n",
    "        C = A * B  # created under name scope <name>\n",
    "    return tf.reduce_sum(C, name=name)  # the operation name will be <name>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Old Fashioned Models\n",
    "Now assume that we are working with the session mode. we now want to see the difference between **tf.layers.dense** and **tf.layers.Dense**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    x = tf.random_normal([10, 20])\n",
    "    y = tf.layers.dense(x, 15)\n",
    "    print(tf.global_variables())\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.random_normal([10, 20])\n",
    "    dense = tf.layers.Dense(15)\n",
    "    print(tf.global_variables())\n",
    "    y = dense(x)\n",
    "    print(tf.global_variables())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see there are three important points:\n",
    "\n",
    "- when we use **tf.layers.dense**, it automatically create variables that are only accessible by name or graph. firstly they can be used once in graph (as a layer), and secondly it is useless to be used in eager execution.\n",
    "- tf.layers.Dense (and any layer) creates its variables not when it is created, but when it is called for the first time. it is an easier way since in many cases we do not know the input size when we are creating a layer.\n",
    "- if we call a layer for second time or more, then it does not create new variables.\n",
    "\n",
    "<div style=\"text-align: justify\">The key in here is using __*tf.get_variable()*__. unlike **tf.Variables()** it does not create a variable, instead it will be create it if the sepecific variable with that name does not exist and if it exists, it will only get it. for more details see [here](https://www.tensorflow.org/api_docs/python/tf/get_variable).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "    return v\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    v1 = foo()  # Creates v.\n",
    "    v2 = foo()  # Gets the same, existing v.\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "    print(v1 is v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trick can be used to create layers, models and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "\n",
    "    def __init__(self, num_units, activation=None, trainable=True, name=None):\n",
    "        if name is None:\n",
    "            name = \"dense\"\n",
    "        self.name = str(name)\n",
    "        self.variable_scope = tf.variable_scope(self.name, reuse=tf.AUTO_REUSE)\n",
    "        self.num_units = num_units\n",
    "        self.trainable = trainable\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, inputs, name=None):\n",
    "        if name is None:\n",
    "            name = \"fully-connected\"\n",
    "        with tf.variable_scope(self.variable_scope):\n",
    "            kernel = tf.get_variable(\"kernel\", shape=[inputs.shape[-1], self.num_units], trainable=self.trainable)\n",
    "            bias = tf.get_variable(\"bias\", shape=[self.num_units], trainable=self.trainable)\n",
    "            with tf.name_scope(name):\n",
    "                output = tf.matmul(inputs, kernel) + bias\n",
    "                if self.activation is not None:\n",
    "                    output = self.activation(output)\n",
    "        return tf.identity(output, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">This method also works when we save and load session in training. that is instead of initializing variables, we load a session and so variables of model or layer are exist in their last weights.</div>\n",
    "\n",
    "<span style=\"color:brown\">How ever this method fails when we are using eager execution:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "    return v\n",
    "\n",
    "\n",
    "v1 = foo()  # Creates v.\n",
    "v2 = foo()  # Creating new v!\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v1 is v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so if we want to use eager execution for debugging, we shoud use two distinct methods **build** and  **call** run build if layer is called for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "    def __init__(self, num_units, activation=None, trainable=True, name=None):\n",
    "        if name is None:\n",
    "            name = \"dense\"\n",
    "        self.name = str(name)\n",
    "        self.variable_scope = tf.variable_scope(self.name, reuse=tf.AUTO_REUSE)\n",
    "        self.num_units = num_units\n",
    "        self.trainable = trainable\n",
    "        self.activation = activation\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        with tf.variable_scope(self.variable_scope):\n",
    "            self.kernel = tf.get_variable(\"kernel\", shape=[inputs_shape[-1], self.num_units], trainable=self.trainable)\n",
    "            self.bias = tf.get_variable(\"bias\", shape=[self.num_units], trainable=self.trainable)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, name=None):\n",
    "        if name is None:\n",
    "            name = \"fully-connected\"\n",
    "        with tf.name_scope(name):\n",
    "            output = tf.matmul(inputs, self.kernel) + self.bias\n",
    "            if self.activation is not None:\n",
    "                output = self.activation(output)\n",
    "        return tf.identity(output, name=name)\n",
    "\n",
    "    def __call__(self, inputs, name=None):\n",
    "        if not self.built:\n",
    "            self.build(inputs.shape)\n",
    "        return self.call(inputs, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling trainability:\n",
    "Tensorflow does not provide easy way to freeze and unfreeze variables like pytorch. the only way is to pass trainable variables throw **var_list** in optimizers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    v = tf.Variable(4.0)\n",
    "    w = tf.Variable(10.0)\n",
    "    init = tf.variables_initializer([v, w])\n",
    "    loss = v*v + w*w\n",
    "    update_v = tf.train.GradientDescentOptimizer(0.05).minimize(loss, var_list=[v])  # update v\n",
    "    update_w = tf.train.GradientDescentOptimizer(0.05).minimize(loss, var_list=[w])  # update w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in order to get trainable variables in a complex model, whether we should use scopes to get variables or use python methods to get variables. we can see a full example in the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def mask_length(length_tensor, max_length, name=None):\n",
    "    \"\"\"\n",
    "    A method which gives the boolean mask tensor related to length_tensor. for length_tensor=[2, 1, 0, 1] and\n",
    "    max_length=3 it returns [[True, True, False], [True, False, False], [False, False, False], [True, False, False]]\n",
    "\n",
    "    Args:\n",
    "        length_tensor: a non-negative integer Tensor with elements less than last dim size\n",
    "        max_length:a Scalar\n",
    "\n",
    "    Returns:\n",
    "         a Tensor of shape [...length_tensor shape..., max_length] of type tf.bool\n",
    "\n",
    "\"\"\"\n",
    "    if name is None:\n",
    "        name = \"max_length\"\n",
    "    with tf.name_scope(name):\n",
    "        length_tensor = tf.expand_dims(length_tensor, -1)\n",
    "        ranges = tf.zeros_like(length_tensor, dtype=length_tensor.dtype) + tf.range(max_length, dtype=length_tensor.dtype)\n",
    "    return tf.less(ranges, length_tensor, name=name)\n",
    "\n",
    "\n",
    "# attention mechanisms\n",
    "def simple_dot_attention(query, key, value, memory_length=None, memory_mask=None, name=None):\n",
    "    \"\"\"\n",
    "    Attention method for given query, keys and values\n",
    "\n",
    "    Args:\n",
    "        query: a Tensor of shape [batch_size, query_dim]\n",
    "        key: a Tensor of shape [batch_size, seq_length, query_dim]\n",
    "        value: a Tensor of shape [batch_size, seq_length, value_dim]\n",
    "        memory_length: (optional) an integer Tensor of shape [batch_size] which specify length of\n",
    "                        memory (key and values) for each sample\n",
    "        memory_mask: (optional) a bool Tensor of shape [batch_size, seq_length] for specifying the true elements of\n",
    "                     memory in the condition that memory_length is not given\n",
    "        name: (optional)\n",
    "\n",
    "    Returns:\n",
    "        a Tensor of shape [batch_size, value_dim] which is the result of attention mechanism\n",
    "\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"simple_dot_attention\"\n",
    "    with tf.name_scope(name):\n",
    "        # handling exceptions\n",
    "        if memory_length is not None and memory_mask is not None:\n",
    "            raise AttributeError(\"Only one of memory_length and memory_mask can be specified\")\n",
    "        query_shape = tf.shape(query)\n",
    "        key_shape = tf.shape(key)\n",
    "        value_shape = tf.shape(value)\n",
    "        batch_size = query_shape[0]\n",
    "        seq_length = key_shape[1]\n",
    "        query_dim = query_shape[1]\n",
    "        if memory_length is not None:\n",
    "            memory_mask = mask_length(memory_length, seq_length)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = tf.fill([batch_size, seq_length], True)\n",
    "        indices = tf.where(memory_mask)\n",
    "        queries = tf.gather(query, indices[:, 0])\n",
    "        keys = tf.boolean_mask(key, memory_mask)\n",
    "        attention_logits = tf.reduce_sum(queries, keys)\n",
    "        attention_logits = tf.scatter_nd(tf.where(memory_mask), attention_logits, [batch_size, seq_length])\n",
    "        attention_logits = tf.where(memory_mask, attention_logits, tf.fill([batch_size, seq_length], -float(\"Inf\")))\n",
    "        attention_coefficients = tf.nn.softmax(attention_logits)\n",
    "        attention = tf.expand_dims(attention_coefficients, -1) * value\n",
    "    return tf.reduce_sum(attention, 1, name=name)\n",
    "\n",
    "\n",
    "def multiple_dot_attention(query, key, value, query_length=None, query_mask=None, memory_length=None,\n",
    "                           memory_mask=None, name=None):\n",
    "    \"\"\"\n",
    "    Attention method for given queries, keys and values, which in each sample we have multiple queries.\n",
    "    (a sequence of queries)\n",
    "\n",
    "    Args:\n",
    "        query: a Tensor of shape [batch_size, q_length, query_dim]\n",
    "        key: a Tensor of shape [batch_size, seq_length, query_dim]\n",
    "        value: a Tensor of shape [batch_size, seq_length, value_dim]\n",
    "        query_length: (optional) an integer Tensor of shape [batch_size] which specify length of\n",
    "                        queries for each sample\n",
    "        query_mask: (optional) a bool Tensor of shape [batch_size, query_length] for specifying  the true\n",
    "                    elements of queries in the condition that query_length is not given\n",
    "        memory_length: (optional) an integer Tensor of shape [batch_size] which specify length of\n",
    "                        memory (key and values) for each sample\n",
    "        memory_mask: (optional) a bool Tensor of shape [batch_size, seq_length] for specifying the true elements of\n",
    "                     keys and values in the condition that memory_length is not given\n",
    "        name: (optional)\n",
    "\n",
    "    Returns:\n",
    "        a Tensor of shape [batch_size, q_length, value_dim] which is the result of attention mechanism\n",
    "\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"multiple_dot_attention\"\n",
    "    with tf.name_scope(name):\n",
    "        if query_length is not None and query_mask is not None:\n",
    "            raise AttributeError(\"Only one of query_length and query_mask can be specified\")\n",
    "        if memory_length is not None and memory_mask is not None:\n",
    "            raise AttributeError(\"Only one of memory_length and memory_mask can be specified\")\n",
    "        query_shape = tf.shape(query)\n",
    "        key_shape = tf.shape(key)\n",
    "        value_shape = tf.shape(value)\n",
    "        batch_size = query_shape[0]\n",
    "        q_length = query_shape[1]\n",
    "        seq_length = key_shape[1]\n",
    "        query_dim = query_shape[2]\n",
    "        value_dim = value_shape[2]\n",
    "        if query_length is not None:\n",
    "            query_mask = mask_length(query_length, q_length)\n",
    "        if query_mask is None:\n",
    "            query_mask = tf.fill([batch_size, q_length], True)\n",
    "        if memory_length is not None:\n",
    "            memory_mask = mask_length(memory_length, seq_length)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = tf.fill([batch_size, seq_length], True)\n",
    "        indices = tf.where(query_mask)\n",
    "        query = tf.boolean_mask(query, query_mask)\n",
    "        key = tf.gather(key, indices[:, 0])\n",
    "        value = tf.gather(value, indices[:, 0])\n",
    "        memory_mask = tf.gather(memory_mask, indices[:, 0])\n",
    "        attention = simple_dot_attention(query, key, value, memory_mask=memory_mask)\n",
    "    return tf.scatter_nd(indices, attention, [batch_size, q_length, value_dim], name=name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=None,\n",
    "                 trainable=True, dtype=tf.float32, name=None):\n",
    "        \"\"\"\n",
    "        Linear dense layer for sequential data.\n",
    "\n",
    "        Args:\n",
    "             units: the size of output representation\n",
    "             activation: (Optional) layer activation\n",
    "             use_bias: (Optional)\n",
    "             kernel_initializer: (Optional)\n",
    "             bias_initializer: (Optional)\n",
    "             trainable: (Optional)\n",
    "             dtype: (Optional)\n",
    "             name: (Optional)\n",
    "\n",
    "        \"\"\"\n",
    "        # general init\n",
    "        if name is None:\n",
    "            name = \"Linear\"\n",
    "        self.name = str(name)\n",
    "        self.variable_scope = tf.variable_scope(name)\n",
    "        self._variables = []\n",
    "        self._trainable_variables = {}\n",
    "        self.built = False\n",
    "\n",
    "        # importing inputs\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self._trainable = bool(trainable)\n",
    "        self._dtype = dtype\n",
    "\n",
    "        # setting attributes\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.inputs_dim = None\n",
    "\n",
    "    @property\n",
    "    def trainable(self):\n",
    "        return self._trainable\n",
    "\n",
    "    @trainable.setter\n",
    "    def trainable(self, t):\n",
    "        assert isinstance(t, bool)\n",
    "        self._trainable = t\n",
    "        for variable in self._trainable_variables:\n",
    "            self._trainable_variables[variable] = t\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._variables\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [var for var in self._variables if self._trainable_variables[var]]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Variables:\n",
    "                kernel: a Tensor of shape [input_dim, self.units]\n",
    "                bias: a Tensor of shape [self.units] if use_bias = True\n",
    "\n",
    "        \"\"\"\n",
    "        self.inputs_dim = input_shape[-1]\n",
    "        with tf.variable_scope(self.variable_scope):\n",
    "            self.kernel = tf.get_variable(name=\"kernel\", shape=[self.inputs_dim, self.units],\n",
    "                                          dtype=self._dtype, initializer=self.kernel_initializer)\n",
    "            self._variables.append(self.kernel)\n",
    "            self._trainable_variables[self.kernel] = self.trainable\n",
    "            if self.use_bias:\n",
    "                self.bias = tf.get_variable(name=\"bias\", shape=[self.units], dtype=self.dtype,\n",
    "                                            initializer=self.bias_initializer)\n",
    "                self._variables.append(self.bias)\n",
    "                self._trainable_variables[self.bias] = self.trainable\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, inputs_num=None, inputs_mask=None, name=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor of shape [batch_size, seq_length, input_dim]\n",
    "            inputs_num: (Optional) an integer Tensor of [batch_size] which specify length of\n",
    "                        inputs for each sample\n",
    "            inputs_mask: (Optional) a bool Tensor of shape [batch_size, seq_length] for specifying  the true\n",
    "                         elements of inputs in the condition that inputs_num is not given\n",
    "            name: (Optional)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        if name is None:\n",
    "            name = \"linear\"\n",
    "\n",
    "        with tf.name_scope(self.name):\n",
    "            if inputs_num is not None and inputs_mask is not None:\n",
    "                raise AttributeError(\"only one of inputs_num or inputs_mask should be specified\")\n",
    "            inputs_shape = tf.shape(inputs)\n",
    "            batch_size = inputs_shape[0]\n",
    "            seq_length = inputs_shape[1]\n",
    "            inputs_dim = inputs_shape[2]\n",
    "            # main code\n",
    "            if inputs_num is not None:\n",
    "                inputs_mask = mask_length(inputs_num, seq_length)\n",
    "            if inputs_mask is None:\n",
    "                inputs_mask = tf.fill([batch_size, seq_length], True)\n",
    "            indices = tf.where(inputs_mask)\n",
    "            inputs = tf.boolean_mask(inputs, inputs_mask)\n",
    "            outputs = tf.matmul(inputs, self.kernel)\n",
    "            if self.bias is not None:\n",
    "                outputs = outputs + self.bias\n",
    "            if self.activation is not None:\n",
    "                outputs = self.activation(outputs)\n",
    "            outputs = tf.scatter_nd(indices, outputs, [batch_size, seq_length, self.units])\n",
    "        return tf.identity(outputs, name=name)\n",
    "\n",
    "    def __call__(self, inputs, inputs_num=None, inputs_mask=None, name=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor of shape [batch_size, seq_length, input_dim]\n",
    "            inputs_num: (Optional) an integer Tensor of [batch_size] which specify length of\n",
    "                        inputs for each sample\n",
    "            inputs_mask: (Optional) a bool Tensor of shape [batch_size, seq_length] for specifying  the true\n",
    "                         elements of inputs in the condition that inputs_num is not given\n",
    "            name: (Optional)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.built:\n",
    "            self.build(inputs.shape)\n",
    "        return self.call(inputs, inputs_num, inputs_mask, name)\n",
    "\n",
    "\n",
    "class SelfAttention(object):\n",
    "    def __init__(self, units, initializer=None, trainable=True, name=None):\n",
    "        \"\"\"\n",
    "        Self attention layer, give a sequence as input and apply bidirectional self-attention mechanism\n",
    "\n",
    "        Args:\n",
    "             units: the representation size of queries and keys\n",
    "             initializer: (Optional) initializer for Query and Key map's weights\n",
    "             trainable: (Optional)\n",
    "             name: (Optional)\n",
    "        \"\"\"\n",
    "        # general init\n",
    "        if name is None:\n",
    "            name = \"self_attention\"\n",
    "        self.name = str(name)\n",
    "        self.variable_scope = tf.variable_scope(self.name)\n",
    "        self.layers = []\n",
    "        self._variables = []\n",
    "        self._trainable_variables = {}\n",
    "        self.built = False\n",
    "\n",
    "        # importing inputs\n",
    "        self.units = units\n",
    "        self.initializer = initializer\n",
    "        self._trainable = bool(trainable)\n",
    "\n",
    "        # setting attributes\n",
    "        with self.variable_scope:\n",
    "            self.key_layer = Linear(self.units, use_bias=False, kernel_initializer=self.initializer,\n",
    "                                    trainable=self.trainable, name=\"key\")\n",
    "            self.query_layer = Linear(self.units, use_bias=False, kernel_initializer=self.initializer,\n",
    "                                      trainable=self.trainable, name=\"query\")\n",
    "            self.layers.append(self.key_layer)\n",
    "            self.layers.append(self.query_layer)\n",
    "        self.inputs_dim = None\n",
    "\n",
    "    @property\n",
    "    def trainable(self):\n",
    "        return self._trainable\n",
    "\n",
    "    @trainable.setter\n",
    "    def trainable(self, t):\n",
    "        t = bool(t)\n",
    "        self._trainable = t\n",
    "        for variable in self._trainable_variables:\n",
    "            self._trainable_variables[variable] = t\n",
    "        for layer in self.layers:\n",
    "            layer.trainable = t\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._variables + [layer.variables for layer in self.layers]\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [var for var in self._variables if self._trainable_variables[var]] +\\\n",
    "               [layer.trainable_variables for layer in self.layers]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.inputs_dim = input_shape[-1]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, inputs_num=None, inputs_mask=None, name=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor of shape [batch_size, seq_length, input_dim]\n",
    "            inputs_num: (Optional) an integer Tensor of [batch_size] which specify length of\n",
    "                        inputs for each sample\n",
    "            inputs_mask: (Optional) a bool Tensor of shape [batch_size, seq_length] for specifying  the true\n",
    "                         elements of inputs in the condition that inputs_num is not given\n",
    "            name: (Optional)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        if name is None:\n",
    "            name = \"self_attention\"\n",
    "\n",
    "        with tf.name_scope(self.name):\n",
    "            key = self.key_layer(inputs, inputs_num, inputs_mask)\n",
    "            query = self.query_layer(inputs, inputs_num, inputs_mask)\n",
    "            result = multiple_dot_attention(query, key, inputs, inputs_num, inputs_mask, inputs_num, inputs_mask)\n",
    "        return tf.identity(result, name=name)\n",
    "\n",
    "    def __call__(self, inputs, inputs_num=None, inputs_mask=None, name=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor of shape [batch_size, seq_length, input_dim]\n",
    "            inputs_num: (Optional) an integer Tensor of [batch_size] which specify length of\n",
    "                        inputs for each sample\n",
    "            inputs_mask: (Optional) a bool Tensor of shape [batch_size, seq_length] for specifying  the true\n",
    "                         elements of inputs in the condition that inputs_num is not given\n",
    "            name: (Optional)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.built:\n",
    "            self.build(inputs.shape)\n",
    "        return self.call(inputs, inputs_num, inputs_mask, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# S.P. standard is to use tf.keras\n",
    "<div style=\"text-align: justify\">\n",
    "Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production, with three key advantages:</div>\n",
    "\n",
    "* _User friendly:_ Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n",
    "* _Modular and composable:_ Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
    "* _Easy to extend:_ Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n",
    "<div style=\"text-align: justify\">\n",
    "The problem with old-fashioned style is that it does not support eager execution completely. for instance if you want to train your model in eager execution mode, you can't save your model easily or load it. Keras supports both eager and session execution perfectly and inhance you to use the modularity facilities of keras such as handling variables and weights automatically (so you won't go throw all hardships we did). Besides many tensorflow's utilites are comming from **tf.keras**; for instance, all layers **tf.layers** are comming from **tf.keras.layers**. It also handle session automatically and we do not need to call session.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu'), \n",
    "                             tf.keras.layers.Dense(64, activation='relu'), \n",
    "                             tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "data = np.random.random((100, 32))\n",
    "labels = np.random.random((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Models and Layers\n",
    "<div style=\"text-align: justify\">All models and layers (those who have variables) should be constructed under **tf.keras.layers.Layer** or **tf.keras.Model**. Keras models and layers will automatically handle variables, weights, trainability and scopes. so in most cases it is only necessary to implement just three methods:</div>\n",
    "* \\_\\_init\\_\\_\n",
    "* build\n",
    "* call\n",
    "\n",
    "Note that many objects are hidden and are named by prefix \"\\_\". etc: **self.\\_name**. here is an implementation of above layers using tf.keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.layers.Dense):\n",
    "    def __init__(self, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=None,\n",
    "                 trainable=True, name=None):\n",
    "        \"\"\"\n",
    "        Linear dense layer for sequential data.\n",
    "\n",
    "        Args:\n",
    "             units: the size of output representation\n",
    "             activation: (Optional) layer activation\n",
    "             use_bias: (Optional)\n",
    "             kernel_initializer: (Optional)\n",
    "             bias_initializer: (Optional)\n",
    "             trainable: (Optional)\n",
    "             name: (Optional)\n",
    "\n",
    "        \"\"\"\n",
    "        # general init\n",
    "        if name is None:\n",
    "            name = \"Linear\"\n",
    "        super().__init__(units, activation, use_bias, kernel_initializer, bias_initializer,\n",
    "                         trainable=trainable, name=name)\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._set_scope()\n",
    "        self._inputs_dim = None\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        self._inputs_dim = inputs_shape[0][-1]\n",
    "        super().build([1, self._inputs_dim])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: whether a Tensor of shape [batch_size, seq_length, input_dim] or a tuple of tensors, first a Tensor\n",
    "            of shape [batch_size, seq_length, input_dim] which is inputs and a Tensor of type tf.bool and size of\n",
    "            [batch_size, seq_length] for specifying the true elements of inputs\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(inputs, tf.Tensor):\n",
    "            inputs_mask = None\n",
    "        else:\n",
    "            inputs, inputs_mask = inputs\n",
    "        inputs_shape = tf.shape(inputs)\n",
    "        batch_size = inputs_shape[0]\n",
    "        seq_length = inputs_shape[1]\n",
    "        inputs_dim = inputs_shape[2]\n",
    "        indices = tf.where(inputs_mask)\n",
    "        inputs = tf.boolean_mask(inputs, inputs_mask)\n",
    "        outputs = super().call(inputs)\n",
    "        outputs = tf.scatter_nd(indices, outputs, [batch_size, seq_length, self.units])\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, initializer=None, trainable=True, name=None):\n",
    "        \"\"\"\n",
    "        Self attention layer, give a sequence as input and apply bidirectional self-attention mechanism\n",
    "\n",
    "        Args:\n",
    "             units: the representation size of queries and keys\n",
    "             initializer: (Optional) initializer for Query and Key map's weights\n",
    "             trainable: (Optional)\n",
    "             name: (Optional)\n",
    "        \"\"\"\n",
    "        # general init\n",
    "        if name is None:\n",
    "            name = \"Linear\"\n",
    "        super().__init__(trainable, name)\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._set_scope()\n",
    "        self._units = units\n",
    "        self._inputs_dim = None\n",
    "        self._layers = []\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.name_scope(self.name):\n",
    "                self._key_dense = Linear(self._units, use_bias=False, kernel_initializer=initializer,\n",
    "                                         trainable=self.trainable, name=\"key_dense\")\n",
    "                self._query_dense = Linear(self._units, use_bias=False, kernel_initializer=initializer,\n",
    "                                           trainable=self.trainable, name=\"query_dense\")\n",
    "        self._layers = [self._key_dense, self._query_dense]\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self._layers.copy()\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return sum([layer.variables for layer in self.layers], [])\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return sum([layer.trainable_variables for layer in self.layers], [])\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: whether a Tensor of shape [batch_size, seq_length, input_dim] or a tuple of tensors, first a Tensor\n",
    "            of shape [batch_size, seq_length, input_dim] which is inputs and a Tensor of type tf.bool and size of\n",
    "            [batch_size, seq_length] for specifying the true elements of inputs\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, seq_length, self.units]\n",
    "\n",
    "        \"\"\"\n",
    "        key = self._key_layer(inputs)\n",
    "        query = self._query_layer(inputs)\n",
    "        if not isinstance(inputs, tf.Tensor):\n",
    "            inputs_mask = None\n",
    "        else:\n",
    "            inputs, inputs_mask = inputs\n",
    "        result = multiple_dot_attention(query, key, inputs, query_mask=inputs_mask, memory_mask=inputs_mask)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a model for bidirectional language model using transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Sequential):\n",
    "    def __init__(self, units, num_blocks, trainable=True, activation=tf.sigmoid, name=None):\n",
    "        \"\"\"\n",
    "        simple transformer with self-attention and linear layers\n",
    "\n",
    "        Args:\n",
    "            units: an integer which shows the output representation\n",
    "            num_blocks: an integer number of num_blocks\n",
    "\n",
    "        \"\"\"\n",
    "        assert num_blocks >= 0\n",
    "        if name is None:\n",
    "            name = \"Linear\"\n",
    "        super().__init__(name=name)\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._set_scope()\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.name_scope(self.name):\n",
    "                for i in range(num_blocks):\n",
    "                    self.add(SelfAttention(units, trainable=trainable, name=\"self_attention\"))\n",
    "                    self.add(Linear(units, activation=activation, trainable=trainable, name=\"dense\"))\n",
    "             \n",
    "            \n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding, num_blocks=3, transformer_units=100,trainable=True, name=None):\n",
    "        \"\"\"\n",
    "        the model consist of an embedding layer, a transformer consist of num_blocks blocks and a softmax layer in the\n",
    "        end which gives the probability vector of the blank word in the sentence. the model uses a trainable vector for\n",
    "        blank embedding.\n",
    "\n",
    "        Args:\n",
    "            embedding: the word embedding for vocabs. a Tensor of shape [vocab_size, embedding_size]\n",
    "            num_blocks: (Optional) an integer which specify the number of transformer blocks\n",
    "            transformer_units: (Optional) an integer which shows the output representation.\n",
    "            trainable: (Optional)\n",
    "            name: (Optional)\n",
    "\n",
    "        \"\"\"\n",
    "        assert num_blocks >= 0\n",
    "        if name is None:\n",
    "            name = \"Linear\"\n",
    "        super().__init__(name=name)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_size = embedding.shape[1]\n",
    "        self._blank_embedding = None\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._set_scope()\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.name_scope(self.name):\n",
    "                embedding_initializer = tf.keras.initializers.constant(embedding)\n",
    "                self._embedding = tf.keras.layers.Embedding(vocab_size, self._embedding_size,\n",
    "                                                            embedding_initializer, trainable=False)\n",
    "                self._transformer = Transformer(transformer_units, num_blocks,\n",
    "                                                trainable=self.trainable, name=\"transformer\")\n",
    "                self._softmax = tf.layers.Dense(vocab_size, tf.nn.softmax, trainable=self.trainable)\n",
    "        self._layers = self._layers + [self._embedding] + [layer for layer in self._transformer.layers] +\\\n",
    "                       [self._softmax]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._blank_embedding = self.add_weight(\"blank_embedding\", [self._embedding_size])\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            inputs: a triple of tensors (inputs, blanks, num_inputs)\n",
    "                    inputs: an integer Tensor of shape [batch_size, seq_length]\n",
    "                    blanks: an integer Tensor of shape [batch_size] which specified the blank word in each sample\n",
    "                    num_inputs: an integer Tensor of shape [batch_size] which specified the length of sequence in\n",
    "                                each sample\n",
    "\n",
    "        Returns:\n",
    "            a Tensor of shape [batch_size, vocab_size] which is the probability vector for blank words\n",
    "\n",
    "        \"\"\"\n",
    "        inputs, blanks, num_inputs = inputs\n",
    "        inputs_shape = tf.shape(inputs)\n",
    "        batch_size = inputs_shape[0]\n",
    "        seq_length = inputs_shape[1]\n",
    "        blanks_indices = tf.concat([tf.range(seq_length), blanks], 1)\n",
    "        blank_mask = tf.scatter_nd(blanks_indices, tf.fill([batch_size]), [batch_size, seq_length])\n",
    "        inputs_mask = mask_length(num_inputs)\n",
    "        x = self._embedding(inputs)\n",
    "        blanks_embedding = tf.zeros_like(x) + self._blank_embedding\n",
    "        x = tf.where(blank_mask, blanks_embedding, x)\n",
    "        x = self._transformer([x, inputs_mask])\n",
    "        probs = self._softmax(x)\n",
    "        return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Some other tips!\n",
    "\n",
    "* Use tf.dataset module to feed data to models. see [here](https://www.tensorflow.org/guide/datasets).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a toy dataset instance:\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(16)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write different parts of the code (model, utils, pre-processing, importing data, ...) in different python files if some of them are complex, in different folders.\n",
    "* Do not specify device for operations in the main code. use **tf.device** in sub branchs in git after debugging the model. Tensorflow will automatically regulate operations.\n",
    "* Take care to use allow_growth in session's config when running on server.\n",
    "* update your github daily or even more frequent!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
